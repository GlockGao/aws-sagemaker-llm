{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f55389-9564-41e7-a8ae-0bbd0904ced3",
   "metadata": {},
   "source": [
    "# ChatGLM-6B-PT\n",
    "\n",
    "本示例介绍 ChatGLM-6B 模型基于 [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) 的微调。P-Tuning v2 将需要微调的参数量减少到原来的 0.1%，再通过模型量化、Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd34a44-77f7-464c-bd98-4147deac0c56",
   "metadata": {
    "tags": []
   },
   "source": [
    "下面以 [ADGEN](https://aclanthology.org/D19-1321.pdf) (广告生成) 数据集为例介绍代码的使用方法。\n",
    "\n",
    "SageMaker Notebook kernel 环境选择 **conda_pytorch_p39**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251967-b1a9-4585-a667-50ed11f18459",
   "metadata": {},
   "source": [
    "## 软件依赖\n",
    "运行微调需要4.27.1版本的`transformers`。除 ChatGLM-6B 的依赖之外，还需要安装以下依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412778b-6d75-41f6-bbd7-66a2d11bdf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rouge_chinese nltk jieba datasets transformers==4.27 sentencepiece cpm_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa318ee0-0010-4e0d-a9d1-d8ecd38e1ea1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 下载数据集\n",
    "ADGEN 数据集任务为根据输入（content）生成一段广告词（summary）。\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"content\": \"类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳\",\n",
    "    \"summary\": \"这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。\"\n",
    "}\n",
    "```\n",
    "\n",
    "从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 ADGEN 数据集，将解压后的 `AdvertiseGen` 目录放到本目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fadcf9-5fc2-4b9d-882a-31ee8ffacd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下载 ADGEN 数据集\n",
    "!wget -O AdvertiseGen.tar.gz https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\n",
    "\n",
    "# 解压数据集\n",
    "!tar -xzvf AdvertiseGen.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29429dfe-5a3f-4a95-83c6-fefd48730a57",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "#### P-Tuning v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9d1b6-f065-45ea-9d6f-08efcce40e3c",
   "metadata": {},
   "source": [
    "下载代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90f46e-7573-4620-8665-5d6401df4b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "rm -rf ChatGLM-6B\n",
    "git clone https://github.com/THUDM/ChatGLM-6B.git\n",
    "cd ChatGLM-6B\n",
    "git checkout 163f94e160f08751545e3722730f1832d73b92d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c14c4-ee95-4b57-9441-7d7efaedd441",
   "metadata": {},
   "source": [
    "运行以下指令进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74796e4-3425-4e3d-9205-d99bcc4ffe11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./ChatGLM-6B/ptuning/train.sh\n",
    "PRE_SEQ_LEN=128\n",
    "LR=2e-2\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python3 main.py \\\n",
    "    --do_train \\\n",
    "    --train_file ../../AdvertiseGen/train.json \\\n",
    "    --validation_file ../../AdvertiseGen/dev.json \\\n",
    "    --prompt_column content \\\n",
    "    --response_column summary \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path THUDM/chatglm-6b \\\n",
    "    --output_dir output/adgen-chatglm-6b-pt-$PRE_SEQ_LEN-$LR \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 64 \\\n",
    "    --max_target_length 64 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps 50 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN \\\n",
    "    --quantization_bit 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f2648-36d1-4ca6-9a19-ba354ebe6c8d",
   "metadata": {},
   "source": [
    "## 开启训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06cae9-5bc4-45ad-8951-119312e87019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ChatGLM-6B/ptuning && bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0de2c5-4223-469d-a8d0-bf35baba5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "16:26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34e6f6-cdf6-4385-b682-b0f4d421f9b0",
   "metadata": {},
   "source": [
    "`train.sh` 中的 `PRE_SEQ_LEN` 和 `LR` 分别是 soft prompt 长度和训练的学习率，可以进行调节以取得最佳的效果。P-Tuning-v2 方法会冻结全部的模型参数，可通过调整 `quantization_bit` 来被原始模型的量化等级，不加此选项则为 FP16 精度加载。\n",
    "\n",
    "在默认配置 `quantization_bit=4`、`per_device_train_batch_size=1`、`gradient_accumulation_steps=16` 下，INT4 的模型参数被冻结，一次训练迭代会以 1 的批处理大小进行 16 次累加的前后向传播，等效为 16 的总批处理大小，此时最低只需 6.7G 显存。若想在同等批处理大小下提升训练效率，可在二者乘积不变的情况下，加大 `per_device_train_batch_size` 的值，但也会带来更多的显存消耗，请根据实际情况酌情调整。\n",
    "\n",
    "如果你想要[从本地加载模型](../README_en.md#load-the-model-locally)，可以将 `train.sh` 中的 `THUDM/chatglm-6b` 改为你本地的模型路径。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218b6d2-9474-4835-89eb-c631b31374b3",
   "metadata": {},
   "source": [
    "### 推理\n",
    "\n",
    "在 P-tuning v2 训练时模型只保存 PrefixEncoder 部分的参数，所以在推理时需要同时加载原 ChatGLM-6B 模型以及 PrefixEncoder 的权重，因此需要指定 `evaluate.sh` 中的参数：\n",
    "\n",
    "```shell\n",
    "--model_name_or_path THUDM/chatglm-6b\n",
    "--ptuning_checkpoint $CHECKPOINT_PATH\n",
    "```\n",
    "\n",
    "仍然兼容旧版全参保存的 Checkpoint，只需要跟之前一样设定 `model_name_or_path`：\n",
    "\n",
    "```shell\n",
    "--model_name_or_path $CHECKPOINT_PATH\n",
    "```\n",
    "\n",
    "评测指标为中文 Rouge score 和 BLEU-4。生成的结果保存在\n",
    "`./output/adgen-chatglm-6b-pt-8-1e-2/generated_predictions.txt`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30e516-fcd7-40d9-99ae-9f1e96267895",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型部署\n",
    "首先载入Tokenizer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4fe34-c754-439d-ba19-38749aabc981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './ChatGLM-6B/ptuning/output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-50/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee9db4-b875-447f-83d0-bed2dfc962f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# 载入Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "# 如果需要加载的是新 Checkpoint（只包含 PrefixEncoder 参数）：\n",
    "config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, pre_seq_len=128)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", config=config, trust_remote_code=True)\n",
    "prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, \"pytorch_model.bin\"))\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "        new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "model = model.quantize(4)\n",
    "model.half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d822b-bc10-4431-8f1f-32236e6461ed",
   "metadata": {},
   "source": [
    "注意你可能需要将 `pre_seq_len` 改成你训练时的实际值。如果你是[从本地加载模型](https://github.com/THUDM/ChatGLM-6B#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B)的话，需要将 `THUDM/chatglm-6b` 改成本地的模型路径（注意不是checkpoint路径）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2096a5-9cb8-4c19-aacc-e3bb440a60b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\"\n",
    "response, history = model.chat(tokenizer, text, history=[])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046f02d-bdb8-4394-8d93-9f8fb565439d",
   "metadata": {},
   "source": [
    "# 5 上传模型到 HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9493-4d4c-43ef-9d6c-e5773aa86427",
   "metadata": {},
   "source": [
    "接下来将 finetune model [上传](https://huggingface.co/docs/huggingface_hub/guides/upload)到 HuggingFace hub，您需要提前注册一个 HuggingFace账号，并生成相应的 token，注意 token 应选择 write 权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce02d7-db34-4d0f-9364-d74255e1f18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce001d-2462-4693-9077-9ce8116e3796",
   "metadata": {},
   "source": [
    "**您需要在 HuggingFace 下提前创建一个新的 model repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5b04f-4b55-4382-ad96-d95f5ae4f40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "folder_path = CHECKPOINT_PATH # \"/path/to/local/model\"\n",
    "repo_id = \"YOUR_HF_NAME/Model_Name\" # \"username/my-cool-space\"\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=folder_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
