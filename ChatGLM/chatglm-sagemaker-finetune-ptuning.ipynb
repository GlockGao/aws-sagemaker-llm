{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4a8ca0-6009-4a1a-81c7-baa7021eaadf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.140)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.140 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.29.140)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.140->boto3) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.140->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.140->boto3) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.159.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.140)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: PyYAML==6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.140 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.29.140)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.140->boto3<2.0,>=1.26.131->sagemaker) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9dfe8c4c-cb83-4306-a823-3b0eb119b47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b28ebafe-953b-4067-b12a-5bebd0a8999c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7fdc1c57ce50>\n",
      "arn:aws:iam::928808346782:role/SageMakerMLOpsRole\n",
      "sagemaker-us-west-2-928808346782\n",
      "928808346782\n",
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "print(sess)\n",
    "print(role)\n",
    "print(sagemaker_default_bucket)\n",
    "print(account)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44baf3b5-8eed-4388-8ffe-005d53a3fa92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM-6B'...\n",
      "Note: switching to '163f94e160f08751545e3722730f1832d73b92d1'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 163f94e Merge pull request #1041 from guoqiangqi/patch-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/ChatGLM-AWS/ChatGLM-6B/ptuning\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "git clone https://github.com/THUDM/ChatGLM-6B.git\n",
    "cd ChatGLM-6B\n",
    "git checkout 163f94e160f08751545e3722730f1832d73b92d1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41c50a39-37e9-4308-b7c0-66693862b729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-25 04:02:36--  https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\n",
      "Resolving cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)... 166.111.6.101, 2402:f000:1:406:166:111:6:101\n",
      "Connecting to cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)|166.111.6.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cloud.tsinghua.edu.cn/seafhttp/files/385463d9-f17a-4cf4-8e5e-1303ab66e660/AdvertiseGen.tar.gz [following]\n",
      "--2023-05-25 04:02:37--  https://cloud.tsinghua.edu.cn/seafhttp/files/385463d9-f17a-4cf4-8e5e-1303ab66e660/AdvertiseGen.tar.gz\n",
      "Reusing existing connection to cloud.tsinghua.edu.cn:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17069994 (16M) [application/octet-stream]\n",
      "Saving to: ‘AdvertiseGen.tar.gz’\n",
      "\n",
      "100%[======================================>] 17,069,994  5.66MB/s   in 2.9s   \n",
      "\n",
      "2023-05-25 04:02:40 (5.66 MB/s) - ‘AdvertiseGen.tar.gz’ saved [17069994/17069994]\n",
      "\n",
      "AdvertiseGen/\n",
      "AdvertiseGen/train.json\n",
      "AdvertiseGen/dev.json\n"
     ]
    }
   ],
   "source": [
    "# 下载 ADGEN 数据集\n",
    "!wget -O AdvertiseGen.tar.gz https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\n",
    "\n",
    "# 解压数据集\n",
    "!tar -xzvf AdvertiseGen.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ead9997-20c9-485e-93e3-20d830974177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/ChatGLM-AWS\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee05fcfc-89a1-45fb-85ea-1681a1316a77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3acd1f6f-6172-4326-8390-3f3a75361898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2247e5f138da47aa80f7c46f1f916e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b278e2b1d1046e29b499206b259672b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27747128d1448c18f6c5bd70cac5ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c167d17d3444a2da0c3c12884b2c4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0f3e9f3e/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03ceef0b1af4a0b8d1116f8f831a862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adae83a8b94645528817fe60498cbdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0a7b12f4049cab0e1548a23e1e099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900a8a23a12450dbc67e6b4c6a2db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f8c5981f214e799db75c66bde8939d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23ca130088e408aaa8e8e238ec290ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce32cfa2431494792cc4bc6f473dd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a933a9cc9514262af18cbc1f248fd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2491fa8c13d49c998dec8ecd9305dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"THUDM/chatglm-6b\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42c0ce80-883b-46bf-a642-e8f69a5fdaab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            # print(os.path.join(root, file))\n",
    "            local_model_path = str(os.path.join(root, file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "088797c9-5995-4c6e-bded-d373e58cad80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/config.json\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model.bin.index.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model.bin.index.json\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/tokenizer_config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/tokenizer_config.json\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/ice_text.model s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/ice_text.model\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00008-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00008-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00007-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00007-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00004-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00004-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00006-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00006-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00001-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00001-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00005-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00005-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00003-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00003-of-00008.bin\n",
      "cp model/models--THUDM--chatglm-6b/snapshots/1d240ba371910e9282298d4592532d7f0f3e9f3e/pytorch_model-00002-of-00008.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00002-of-00008.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/chatglm/original-6B/\n",
    "./s5cmd sync ./AdvertiseGen/ s3://${sagemaker_default_bucket}/llm/datasets/chatglm/AdvertiseGen/\n",
    "\n",
    "rm -rf model\n",
    "rm -rf AdvertiseGen\n",
    "rm -rf AdvertiseGen.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd6bd111-8e53-49be-9a3f-650a0914e333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dd417aa-4130-405a-9e85-60505f1c80a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
       " 'TRAIN_DATASET': '/opt/ml/input/data/AdvertiseGen/train.json',\n",
       " 'TEST_DATASET': '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
       " 'PROMPT_COLUMN': 'content',\n",
       " 'RESPONSE_COLUMN': 'summary',\n",
       " 'MODEL_NAME_OR_PATH': 's3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/',\n",
       " 'OUTPUT_DIR': '/opt/ml/model/adgen-chatglm-6b-ft',\n",
       " 'MODEL_OUTPUT_S3_PATH': 's3://sagemaker-us-west-2-928808346782/llm/models/chatglm/ptuning-adgen/',\n",
       " 'TRAIN_STEPS': '50'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87c80278-69f6-42c8-a15e-22e64d240b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM-6B/ptuning/arguments.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM-6B/ptuning/arguments.py\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_output_s3_path: str = field(\n",
    "        metadata={\"help\": \"Path to model saved in s3 path using s5cmd utily\"}\n",
    "    )\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    ptuning_checkpoint: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to p-tuning v2 checkpoints\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    resize_position_embeddings: Optional[bool] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n",
    "                \"the model's position embeddings.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    quantization_bit: Optional[int] = field(\n",
    "        default=None\n",
    "    )\n",
    "    pre_seq_len: Optional[int] = field(\n",
    "        default=None\n",
    "    )\n",
    "    prefix_projection: bool = field(\n",
    "        default=False\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    prompt_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
    "    )\n",
    "    response_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
    "    )\n",
    "    history_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the history of chat.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "                \"during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to model maximum sentence length. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "                \"efficient on GPU but very bad for TPU.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "                \"which is used during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
    "    )\n",
    "\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to force as the first generated token after the decoder_start_token_id.\"\n",
    "                \"Useful for multilingual models like mBART where the first generated token\"\n",
    "                \"needs to be the target language token (Usually it is the target language token)\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None and self.test_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation/test file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21105576-92e1-4ca6-895d-bf22b3fbc1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ChatGLM-6B/ptuning/sm_ptune_train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM-6B/ptuning/sm_ptune_train.sh\n",
    "\n",
    "PRE_SEQ_LEN=128 && LR=2e-2 && CUDA_VISIBLE_DEVICES=0 python3 main.py \\\n",
    "    --do_train \\\n",
    "    --train_file $TRAIN_DATASET \\\n",
    "    --validation_file $TEST_DATASET \\\n",
    "    --prompt_column ${PROMPT_COLUMN} \\\n",
    "    --response_column ${RESPONSE_COLUMN}  \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n",
    "    --model_output_s3_path ${MODEL_OUTPUT_S3_PATH} \\\n",
    "    --output_dir ${OUTPUT_DIR} \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 64 \\\n",
    "    --max_target_length 64 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps ${TRAIN_STEPS} \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps ${TRAIN_STEPS} \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN \\\n",
    "    --quantization_bit 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4ef9be6-cc81-4826-91be-db92380b1d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM-6B/ptuning/sm_ptune_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM-6B/ptuning/sm_ptune_train.py\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = str(os.environ['PYTORCH_CUDA_ALLOC_CONF'])\n",
    "    os.environ['LD_LIBRARY_PATH'] = str(os.environ['LD_LIBRARY_PATH'])\n",
    "    os.environ['TRAIN_DATASET'] = str(os.environ['TRAIN_DATASET'])\n",
    "    os.environ['TEST_DATASET'] = str(os.environ['TEST_DATASET'])\n",
    "    os.environ['PROMPT_COLUMN'] = str(os.environ['PROMPT_COLUMN'])\n",
    "    os.environ['RESPONSE_COLUMN'] = str(os.environ['RESPONSE_COLUMN'])\n",
    "    os.environ['MODEL_NAME_OR_PATH'] = str(os.environ['MODEL_NAME_OR_PATH'])\n",
    "    os.environ['OUTPUT_DIR'] = str(os.environ['OUTPUT_DIR'])\n",
    "    os.environ['MODEL_OUTPUT_S3_PATH'] = str(os.environ['MODEL_OUTPUT_S3_PATH'])\n",
    "\n",
    "    # os.system(\"chmod +x ./s5cmd\")\n",
    "    os.system(\"/bin/bash sm_ptune_train.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f848ff4-6d25-4d7c-be52-fe045aa9abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM-6B/ptuning/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM-6B/ptuning/main.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for sequence to sequence.\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import jieba \n",
    "from rouge_chinese import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from trainer_seq2seq import Seq2SeqTrainer\n",
    "\n",
    "from arguments import ModelArguments, DataTrainingArguments\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    if training_args.should_log:\n",
    "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    # datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load dataset\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.validation_file.split(\".\")[-1]\n",
    "    if data_args.test_file is not None:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "        extension = data_args.test_file.split(\".\")[-1]\n",
    "\n",
    "    raw_datasets = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    \n",
    "    # if s3 path model, use s5cmd to download the model to /tmp/orignal/ for model load\n",
    "    print(\"#\" * 100)\n",
    "    print(\"model_name_or_path:{}\".format(model_args.model_name_or_path))\n",
    "    if \"s3\" in model_args.model_name_or_path:\n",
    "        print(\"*\" * 50)\n",
    "        os.system(\"pwd\")\n",
    "        os.system(\"ls -l\")\n",
    "        os.system(\"cp ./s5cmd  /tmp/ && chmod +x /tmp/s5cmd\")\n",
    "        os.system(\"/tmp/s5cmd sync {0} {1}\".format(model_args.model_name_or_path + \"*\", \"/tmp/orignal/\"))\n",
    "        model_args.model_name_or_path = \"/tmp/orignal/\"\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    config.pre_seq_len = model_args.pre_seq_len\n",
    "    config.prefix_projection = model_args.prefix_projection\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "    if model_args.ptuning_checkpoint is not None:\n",
    "        # Evaluation\n",
    "        # Loading extra state dict of prefix encoder\n",
    "        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)\n",
    "        prefix_state_dict = torch.load(os.path.join(model_args.ptuning_checkpoint, \"pytorch_model.bin\"))\n",
    "        new_prefix_state_dict = {}\n",
    "        for k, v in prefix_state_dict.items():\n",
    "            if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)\n",
    "\n",
    "    if model_args.quantization_bit is not None:\n",
    "        print(f\"Quantized to {model_args.quantization_bit} bit\")\n",
    "        model = model.quantize(model_args.quantization_bit)\n",
    "    if model_args.pre_seq_len is not None:\n",
    "        # P-tuning v2\n",
    "        model = model.half()\n",
    "        model.transformer.prefix_encoder.float()\n",
    "    else:\n",
    "        # Finetune\n",
    "        model = model.float()\n",
    "\n",
    "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize inputs and targets.\n",
    "    if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "    elif training_args.do_eval:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "    elif training_args.do_predict:\n",
    "        column_names = raw_datasets[\"test\"].column_names\n",
    "    else:\n",
    "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
    "        return\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    prompt_column = data_args.prompt_column\n",
    "    response_column = data_args.response_column\n",
    "    history_column = data_args.history_column\n",
    "    \n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = data_args.max_target_length\n",
    "\n",
    "    def preprocess_function_eval(examples):\n",
    "        inputs, targets = [], []\n",
    "        for i in range(len(examples[prompt_column])):\n",
    "            if examples[prompt_column][i] and examples[response_column][i]:\n",
    "                query = examples[prompt_column][i]\n",
    "                if history_column is None or len(examples[history_column][i]) == 0:\n",
    "                    prompt = query\n",
    "                else:\n",
    "                    prompt = \"\"\n",
    "                    history = examples[history_column][i]\n",
    "                    for turn_idx, (old_query, response) in enumerate(history):\n",
    "                        prompt += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(turn_idx, old_query, response)\n",
    "                    prompt += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n",
    "                inputs.append(prompt)\n",
    "                targets.append(examples[response_column][i])\n",
    "\n",
    "        inputs = [prefix + inp for inp in inputs]\n",
    "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, truncation=True, padding=True)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def preprocess_function_train(examples):\n",
    "        max_seq_length = data_args.max_source_length + data_args.max_target_length\n",
    "\n",
    "        model_inputs = {\n",
    "            \"input_ids\": [],\n",
    "            \"labels\": [],\n",
    "        }\n",
    "        for i in range(len(examples[prompt_column])):\n",
    "            if examples[prompt_column][i] and examples[response_column][i]:\n",
    "                query, answer = examples[prompt_column][i], examples[response_column][i]\n",
    "\n",
    "                if history_column is None:\n",
    "                    prompt = query\n",
    "                else:\n",
    "                    prompt = \"\"\n",
    "                    history = examples[history_column][i]\n",
    "                    for turn_idx, (old_query, response) in enumerate(history):\n",
    "                        prompt += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(turn_idx, old_query, response)\n",
    "                    prompt += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n",
    "\n",
    "                prompt = prefix + prompt\n",
    "                a_ids = tokenizer.encode(text=prompt, add_special_tokens=False)\n",
    "                b_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
    "\n",
    "                if len(a_ids) > data_args.max_source_length - 1:\n",
    "                    a_ids = a_ids[: data_args.max_source_length - 1]\n",
    "\n",
    "                if len(b_ids) > data_args.max_target_length - 2:\n",
    "                    b_ids = b_ids[: data_args.max_target_length - 2]\n",
    "\n",
    "                input_ids = tokenizer.build_inputs_with_special_tokens(a_ids, b_ids)\n",
    "\n",
    "                context_length = input_ids.index(tokenizer.bos_token_id)\n",
    "                mask_position = context_length - 1\n",
    "                labels = [-100] * context_length + input_ids[mask_position+1:]\n",
    "                \n",
    "                pad_len = max_seq_length - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "                labels = labels + [tokenizer.pad_token_id] * pad_len\n",
    "                if data_args.ignore_pad_token_for_loss:\n",
    "                    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "\n",
    "                model_inputs[\"input_ids\"].append(input_ids)\n",
    "                model_inputs[\"labels\"].append(labels)\n",
    "\n",
    "        return model_inputs\n",
    "    \n",
    "    def print_dataset_example(example):\n",
    "        print(\"input_ids\",example[\"input_ids\"])\n",
    "        print(\"inputs\", tokenizer.decode(example[\"input_ids\"]))\n",
    "        print(\"label_ids\", example[\"labels\"])\n",
    "        print(\"labels\", tokenizer.decode(example[\"labels\"]))\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "            train_dataset = train_dataset.map(\n",
    "                preprocess_function_train,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "        print_dataset_example(train_dataset[0])\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        max_target_length = data_args.val_max_target_length\n",
    "        if \"validation\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = raw_datasets[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "            eval_dataset = eval_dataset.map(\n",
    "                preprocess_function_eval,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "        print_dataset_example(eval_dataset[0])\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        max_target_length = data_args.val_max_target_length\n",
    "        if \"test\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        predict_dataset = raw_datasets[\"test\"]\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
    "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "            predict_dataset = predict_dataset.map(\n",
    "                preprocess_function_eval,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on prediction dataset\",\n",
    "            )\n",
    "        print_dataset_example(predict_dataset[0])\n",
    "\n",
    "    # Data collator\n",
    "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=None,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # Metric\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        score_dict = {\n",
    "            \"rouge-1\": [],\n",
    "            \"rouge-2\": [],\n",
    "            \"rouge-l\": [],\n",
    "            \"bleu-4\": []\n",
    "        }\n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            hypothesis = list(jieba.cut(pred))\n",
    "            reference = list(jieba.cut(label))\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(' '.join(hypothesis) , ' '.join(reference))\n",
    "            result = scores[0]\n",
    "            \n",
    "            for k, v in result.items():\n",
    "                score_dict[k].append(round(v[\"f\"] * 100, 4))\n",
    "            bleu_score = sentence_bleu([list(label)], list(pred), smoothing_function=SmoothingFunction().method3)\n",
    "            score_dict[\"bleu-4\"].append(round(bleu_score * 100, 4))\n",
    "\n",
    "        for k, v in score_dict.items():\n",
    "            score_dict[k] = float(np.mean(v))\n",
    "        return score_dict\n",
    "\n",
    "    # Override the decoding parameters of Seq2SeqTrainer\n",
    "    training_args.generation_max_length = (\n",
    "        training_args.generation_max_length\n",
    "        if training_args.generation_max_length is not None\n",
    "        else data_args.val_max_target_length\n",
    "    )\n",
    "    training_args.generation_num_beams = (\n",
    "        data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
    "    )\n",
    "    # Initialize our Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
    "        save_prefixencoder=model_args.pre_seq_len is not None\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        # elif last_checkpoint is not None:\n",
    "        #     checkpoint = last_checkpoint\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.enable_input_require_grads()\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        # trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        max_train_samples = (\n",
    "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        \n",
    "        print(\"------saving model!-----\")\n",
    "        \n",
    "        save_model_dir = os.environ['OUTPUT_DIR']\n",
    "        tokenizer.save_pretrained(save_model_dir)\n",
    "        trainer.save_model(save_model_dir)\n",
    "        print(\"save_model_dir:{}\".format(save_model_dir))\n",
    "        print(\"------model is saved!-----\")\n",
    "        \n",
    "        os.system(\"pwd\")\n",
    "        os.system(\"ls -l\")\n",
    "        \n",
    "        os.system(\"./s5cmd sync {0} {1}\".format(save_model_dir, os.environ['MODEL_OUTPUT_S3_PATH']))\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    max_seq_length = data_args.max_source_length + data_args.max_target_length + 1\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(metric_key_prefix=\"eval\", do_sample=True, top_p=0.7, max_length=max_seq_length, temperature=0.95)\n",
    "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        logger.info(\"*** Predict ***\")\n",
    "        predict_results = trainer.predict(predict_dataset, metric_key_prefix=\"predict\", max_length=max_seq_length, do_sample=True, top_p=0.7, temperature=0.95)\n",
    "        metrics = predict_results.metrics\n",
    "        max_predict_samples = (\n",
    "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "        )\n",
    "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"predict\", metrics)\n",
    "        trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "        if trainer.is_world_process_zero():\n",
    "            if training_args.predict_with_generate:\n",
    "                predictions = tokenizer.batch_decode(\n",
    "                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                predictions = [pred.strip() for pred in predictions]\n",
    "                labels = tokenizer.batch_decode(\n",
    "                    predict_results.label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                labels = [label.strip() for label in labels]\n",
    "                output_prediction_file = os.path.join(training_args.output_dir, \"generated_predictions.txt\")\n",
    "                with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "                    for p, l in zip(predictions, labels):\n",
    "                        res = json.dumps({\"labels\": l, \"predict\": p}, ensure_ascii=False)\n",
    "                        writer.write(f\"{res}\\n\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc67f35a-28d4-4f38-bb65-196b188feda4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ChatGLM-6B/ptuning/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM-6B/ptuning/requirements.txt\n",
    "\n",
    "protobuf\n",
    "#git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "transformers==4.28.0\n",
    "cpm_kernels\n",
    "torch>=1.10\n",
    "gradio\n",
    "mdtex2html\n",
    "sentencepiece\n",
    "accelerate\n",
    "datasets\n",
    "huggingface\n",
    "jieba\n",
    "rouge_chinese\n",
    "nltk\n",
    "deepspeed==0.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a8e27b9-97f1-48f7-bd89-4628177cfb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod +x s5cmd\n",
    "! cp s5cmd ChatGLM-6B/ptuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eea75cc1-c337-4ac6-9f93-e3d797acabb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Training Job Name\n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = f'huggingface-chatglm-finetune-ptuning-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "instance_type  = \"ml.g4dn.2xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "# 基础模型存放地址\n",
    "# model_name_or_path = 'THUDM/chatglm-6b'\n",
    "model_name_or_path = 's3://{}/llm/models/chatglm/original-6B/'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 微调模型输出地址\n",
    "output_dir         = '/opt/ml/model/adgen-chatglm-6b-ft'\n",
    "model_s3_path      = 's3://{}/llm/models/chatglm/finetune-ptuning-adgen/'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 模型环境变量设置\n",
    "environment = {\n",
    "    'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "    'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "    'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "    'PROMPT_COLUMN'          : 'content',\n",
    "    'RESPONSE_COLUMN'        : 'summary',\n",
    "    'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "    'OUTPUT_DIR'             : output_dir,\n",
    "    'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "    'TRAIN_STEPS'            : '50'\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "   'AdvertiseGen': f\"s3://{sagemaker_default_bucket}/llm/datasets/chatglm/AdvertiseGen/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "477cc154-af4c-480d-8f8a-d69fe3eca03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_ptune_train.py',\n",
    "    source_dir           = './ChatGLM-6B/ptuning',\n",
    "    instance_type        = instance_type,\n",
    "    instance_count       = instance_count,\n",
    "    base_job_name        = job_name,\n",
    "    role                 = role,\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',\n",
    "    pytorch_version      = '1.13',\n",
    "    py_version           = 'py39',\n",
    "    environment          = environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2e0b9d1-7366-481e-a72e-b75f497221bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-25 04:57:28 Starting - Starting the training job...\n",
      "2023-05-25 04:57:43 Starting - Preparing the instances for training......\n",
      "2023-05-25 04:58:55 Downloading - Downloading input data\n",
      "2023-05-25 04:58:55 Training - Downloading the training image..................\n",
      "2023-05-25 05:01:36 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:05,058 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:05,077 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:05,088 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:05,090 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:05,568 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 20.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.32.0-py3-none-any.whl (19.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/19.9 MB 60.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 63.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 102.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.3\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.3.tar.gz (765 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 87.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 16)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 16)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 16)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 16)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 16)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 6)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 43.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.2-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 15.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.8/135.8 kB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 22.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 66.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.2.4\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.5-py3-none-any.whl (288 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.1/288.1 kB 61.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 41.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.76.0-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 21.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 8)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 14)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 15)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 7)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 7)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 7)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 7)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 20.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 22.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.2-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.5/72.5 kB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 8)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (0.11.0)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 25.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 8)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 7)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776404 sha256=97d0470de0d7fbc7c07b4532f6ace39922f31ff668fe366b81726fe3dc8a9992\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=2eb11b8d555c7357732c824717760b039d201f40230269f7be0490f6f294e4c5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=92d4c9a9b4d3dd175567c67a81abb3b7d7bcfef25ccffd04182e283b471a5ac5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.3 fastapi-0.95.2 ffmpy-0.3.0 gradio-3.32.0 gradio-client-0.2.5 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.76.0 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.13 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 transformers-4.28.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,607 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,607 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,628 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,658 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,687 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:33,698 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-928808346782/huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_ptune_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_ptune_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_ptune_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_ptune_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-928808346782/huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-928808346782/huggingface-chatglm-finetune-ptuning-20-2023-05-25-04-57-25-030/source/sourcedir.tar.gz\",\"module_name\":\"sm_ptune_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_ptune_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 sm_ptune_train.py\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:02:38.230: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:38,234 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:02:38,257 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m05/25/2023 05:02:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/25/2023 05:02:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_config=None,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=4,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.02,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/adgen-chatglm-6b-ft/runs/May25_05-02-43_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=50,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=50,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/25/2023 05:02:44 - WARNING - datasets.builder - Using custom data configuration default-c57f8cdd3405492d\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c57f8cdd3405492d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 12710.01it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1855.07it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 111750 examples [00:00, 926237.99 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c57f8cdd3405492d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 516.54it/s]\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/MODEL_LICENSE /tmp/orignal/MODEL_LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/configuration_chatglm.py /tmp/orignal/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/quantization.py /tmp/orignal/quantization.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/test_modeling_chatglm.py /tmp/orignal/test_modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/LICENSE /tmp/orignal/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/modeling_chatglm.py /tmp/orignal/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/config.json /tmp/orignal/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/tokenizer_config.json /tmp/orignal/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model.bin.index.json /tmp/orignal/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/tokenization_chatglm.py /tmp/orignal/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/.gitattributes /tmp/orignal/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/README.md /tmp/orignal/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/ice_text.model /tmp/orignal/ice_text.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00007-of-00008.bin /tmp/orignal/pytorch_model-00007-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00008-of-00008.bin /tmp/orignal/pytorch_model-00008-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00005-of-00008.bin /tmp/orignal/pytorch_model-00005-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00003-of-00008.bin /tmp/orignal/pytorch_model-00003-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00002-of-00008.bin /tmp/orignal/pytorch_model-00002-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00004-of-00008.bin /tmp/orignal/pytorch_model-00004-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00006-of-00008.bin /tmp/orignal/pytorch_model-00006-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/pytorch_model-00001-of-00008.bin /tmp/orignal/pytorch_model-00001-of-00008.bin\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-05-25 05:03:37,765 >> loading configuration file /tmp/orignal/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-05-25 05:03:37,765 >> loading configuration file /tmp/orignal/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-25 05:03:37,765 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-25 05:03:37,765 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-05-25 05:03:37,770 >> loading configuration file /tmp/orignal/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-05-25 05:03:37,770 >> loading configuration file /tmp/orignal/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-25 05:03:37,771 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"/tmp/orignal/\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-25 05:03:37,771 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"/tmp/orignal/\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-25 05:03:37,786 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-25 05:03:37,786 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,801 >> loading file ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,801 >> loading file ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-05-25 05:03:37,802 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-25 05:03:38,036 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-25 05:03:38,036 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-05-25 05:03:38,064 >> loading weights file /tmp/orignal/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-05-25 05:03:38,064 >> loading weights file /tmp/orignal/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-25 05:03:38,081 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-25 05:03:38,081 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:04,  2.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:02,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:16<00:00,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:16<00:00,  2.08s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-25 05:03:55,110 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-25 05:03:55,110 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-05-25 05:03:55,110 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /tmp/orignal/ and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-05-25 05:03:55,110 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /tmp/orignal/ and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-25 05:03:55,156 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-25 05:03:55,156 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mQuantized to 4 bit\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:26,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:08<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:18,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:17,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:16,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:15,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:17<01:14,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:14,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:13,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:12,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:11,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:21<01:10,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:10,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:26<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:30<01:02,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:01,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:00,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:58,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:35<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:55,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:54,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:53,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:39<00:52,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:52,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:51,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:43<00:49,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:48,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:47,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:46,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:47<00:45,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:48<00:44,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:43,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:42,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:41,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:51<00:41,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:52<00:40,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:56<00:36,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:57<00:35,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:00<00:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:01<00:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:29,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:04<00:28,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:05<00:27,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:06<00:26,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:25,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:24,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:08<00:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:09<00:23,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:10<00:22,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:21,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:20,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:13<00:19,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:14<00:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:17<00:15,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:18<00:14,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:19<00:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:21<00:11,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:22<00:10,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:23<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:26<00:06,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:27<00:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:28<00:04,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:30<00:02,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:31<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:32<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.24ba/s]\u001b[0m\n",
      "\u001b[34minput_ids\u001b[0m\n",
      "\u001b[34m[5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.592: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.631 algo-1:157 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.665 algo-1:157 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.666 algo-1:157 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.667 algo-1:157 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.667 algo-1:157 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-25 05:07:38.667 algo-1:157 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m05/25/2023 05:07:39 - WARNING - transformers_modules.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/50 [00:08<06:41,  8.19s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:13<05:19,  6.66s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:19<04:50,  6.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:24<04:33,  5.95s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:30<04:22,  5.83s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:36<04:13,  5.76s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:41<04:06,  5.73s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:47<03:59,  5.71s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:53<03:53,  5.69s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:58<03:47,  5.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.6184, 'learning_rate': 0.016, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:58<03:47,  5.68s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [01:04<03:41,  5.67s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [01:10<03:35,  5.66s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [01:15<03:29,  5.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [01:21<03:23,  5.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [01:27<03:17,  5.66s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [01:32<03:12,  5.66s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [01:38<03:06,  5.65s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [01:44<03:00,  5.65s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [01:49<02:54,  5.64s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [01:55<02:49,  5.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.2772, 'learning_rate': 0.012, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [01:55<02:49,  5.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [02:00<02:43,  5.64s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [02:06<02:37,  5.64s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [02:12<02:32,  5.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [02:17<02:26,  5.64s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [02:23<02:21,  5.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [02:29<02:15,  5.64s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [02:34<02:09,  5.65s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [02:40<02:04,  5.65s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [02:46<01:58,  5.65s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [02:51<01:53,  5.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.1324, 'learning_rate': 0.008, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [02:51<01:53,  5.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [02:57<01:47,  5.65s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [03:03<01:41,  5.65s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [03:08<01:36,  5.66s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [03:14<01:30,  5.66s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [03:20<01:24,  5.66s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [03:25<01:19,  5.67s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [03:31<01:13,  5.67s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [03:37<01:08,  5.67s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [03:42<01:02,  5.68s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [03:48<00:56,  5.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.0109, 'learning_rate': 0.004, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [03:48<00:56,  5.69s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [03:54<00:51,  5.69s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [03:59<00:45,  5.69s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [04:05<00:39,  5.70s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [04:11<00:34,  5.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [04:17<00:28,  5.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [04:22<00:22,  5.72s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [04:28<00:17,  5.73s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [04:34<00:11,  5.74s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [04:40<00:05,  5.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [04:45<00:00,  5.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.9951, 'learning_rate': 0.0, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [04:45<00:00,  5.75s/it]\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-25 05:12:24,375 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-25 05:12:24,375 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-25 05:12:24,375 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-25 05:12:24,375 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-25 05:12:24,569 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-25 05:12:24,569 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:24,570 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:24,570 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:24,571 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:24,571 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'train_runtime': 286.5025, 'train_samples_per_second': 2.792, 'train_steps_per_second': 0.175, 'train_loss': 5.20681640625, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [04:46<00:00,  5.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [04:46<00:00,  5.73s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =       0.01\u001b[0m\n",
      "\u001b[34mtrain_loss               =     5.2068\n",
      "  train_runtime            = 0:04:46.50\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      2.792\n",
      "  train_steps_per_second   =      0.175\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:24,964 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:24,964 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:24,964 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:24,964 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-25 05:12:24,973 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-25 05:12:24,973 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-25 05:12:24,973 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-25 05:12:24,973 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-25 05:12:25,166 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-25 05:12:25,166 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:25,167 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-25 05:12:25,167 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:25,167 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-25 05:12:25,167 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34msave_model_dir:/opt/ml/model/adgen-chatglm-6b-ft\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34mtotal 12428\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000    10804 May 25 03:40 README.md\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000    11455 May 25 03:40 README_en.md\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000     8606 May 25 04:33 arguments.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      509 May 25 03:40 deepspeed.json\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      766 May 25 03:40 ds_train_finetune.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      660 May 25 03:40 evaluate.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      562 May 25 03:40 evaluate_finetune.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000    19450 May 25 04:36 main.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      255 May 25 04:57 requirements.txt\u001b[0m\n",
      "\u001b[34m-rwxrwxr-x 1 1000 1000 12419072 May 25 04:39 s5cmd\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      769 May 25 04:49 sm_ptune_train.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      809 May 25 04:34 sm_ptune_train.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      753 May 25 03:40 train.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      745 May 25 03:40 train_chat.sh\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000   185598 May 25 03:40 trainer.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000    11496 May 25 03:40 trainer_seq2seq.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000     5684 May 25 03:40 web_demo.py\u001b[0m\n",
      "\u001b[34m-rw-rw-r-- 1 1000 1000      217 May 25 03:40 web_demo.sh\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/configuration_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/all_results.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/all_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/training_args.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/tokenization_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/quantization.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/scheduler.pt s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/scheduler.pt\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/modeling_chatglm.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/quantization.py s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/train_results.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/train_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state.pth s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/rng_state.pth\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/trainer_state.json s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/ice_text.model s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/optimizer.pt s3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/optimizer.pt\u001b[0m\n",
      "\u001b[34m2023-05-25 05:12:27,927 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:12:27,927 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-25 05:12:27,927 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-25 05:12:34 Uploading - Uploading generated training model\n",
      "2023-05-25 05:13:35 Completed - Training job completed\n",
      "Training seconds: 899\n",
      "Billable seconds: 899\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97504aea-de41-46d1-93c1-39de525e34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d74b206e-6cd0-4b0c-8a91-0d924699f5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45e9ad54-e527-4e26-9d53-6cf3f6bd316a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west-2'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5da1ff4d-cc76-42fc-a711-8f822aed6a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "upload: ./model.tar.gz to s3://sagemaker-us-west-2-928808346782/chatglm/assets/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'chatglm')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'chatglm')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "00d2c840-4ed1-4874-84c4-6c0309f3c552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name                  = None\n",
    "entry_point                 = 'chatglm-inference-finetune.py'\n",
    "framework_version           = '1.13.1'\n",
    "py_version                  = 'py39'\n",
    "base_model_name_or_path     = 's3://{}/llm/models/chatglm/original-6B/'.format(sagemaker_default_bucket)\n",
    "finetune_model_name_or_path = 's3://{}/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 模型环境变量设置\n",
    "model_environment  = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': '600',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
    "    'MODEL_NAME_OR_PATH'            : base_model_name_or_path,\n",
    "    'PRE_SEQ_LEN'                   : '128',\n",
    "    'FINETUNE_MODEL_NAME_OR_PATH'   : finetune_model_name_or_path,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "076681b6-3064-4dd5-b9ca-83cedab9da2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name              = model_name,\n",
    "    model_data        = model_data,\n",
    "    entry_point       = entry_point,\n",
    "    source_dir        = './code',\n",
    "    role              = role,\n",
    "    framework_version = framework_version, \n",
    "    py_version        = py_version,\n",
    "    env               = model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "439eaf8f-8a78-43f0-a4ff-d6e80992e90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SAGEMAKER_MODEL_SERVER_TIMEOUT': '600',\n",
       " 'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       " 'MODEL_NAME_OR_PATH': 's3://sagemaker-us-west-2-928808346782/llm/models/chatglm/original-6B/',\n",
       " 'PRE_SEQ_LEN': '128',\n",
       " 'FINETUNE_MODEL_NAME_OR_PATH': 's3://sagemaker-us-west-2-928808346782/llm/models/chatglm/finetune-ptuning-adgen/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "92557cf7-950a-46ca-b58c-2e239bb02207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-west-2-928808346782/chatglm/assets/model.tar.gz), script artifact (./code), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-2-928808346782/pytorch-inference-2023-05-25-15-08-58-978/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2023-05-25-15-09-10-875\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-inference-2023-05-25-15-09-11-545\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-inference-2023-05-25-15-09-11-545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name         = None\n",
    "instance_type         = 'ml.g4dn.2xlarge'\n",
    "instance_count        = 1\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name          = endpoint_name,\n",
    "    instance_type          = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer             = JSONSerializer(),\n",
    "    deserializer           = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b202bd61-2565-4ff8-b79a-811742391218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这件外套采用牛仔布制作，看起来简单大方，很有气质。搭配白色，很有小清新的感觉。采用了破洞的设计，凸显了时尚的风格。同时，搭配刺绣图案，更加亮眼。\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"ask\": \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f8f17-d43a-464b-99f4-a745b9f8841b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
